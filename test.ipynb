{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import lib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/marinneyret/Desktop/LLMapp/.venv/lib/python3.11/site-packages/threadpoolctl.py:1214: RuntimeWarning: \n",
      "Found Intel OpenMP ('libiomp') and LLVM OpenMP ('libomp') loaded at\n",
      "the same time. Both libraries are known to be incompatible and this\n",
      "can cause random crashes or deadlocks on Linux when loaded in the\n",
      "same Python program.\n",
      "Using threadpoolctl may cause crashes or deadlocks. For more\n",
      "information and possible workarounds, please see\n",
      "    https://github.com/joblib/threadpoolctl/blob/master/multiple_openmp.md\n",
      "\n",
      "  warnings.warn(msg, RuntimeWarning)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_huggingface import HuggingFaceEndpoint\n",
    "from langchain import PromptTemplate, LLMChain\n",
    "from transformers import pipeline\n",
    "import tensorflow as tf\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "hf_api = os.getenv(\"HF_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HF Endpoint API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/marinneyret/Desktop/LLMapp/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: read).\n",
      "Your token has been saved to /Users/marinneyret/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'?\\nRe: is victoria a lovely name??\\nI think Victoria is a beautiful and elegant name. It has a regal and sophisticated feel to it, which is fitting given the history of the name (Victoria being the name of the British monarch'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm = HuggingFaceEndpoint(\n",
    "    repo_id=\"meta-llama/Meta-Llama-3-8B-Instruct\",\n",
    "    task=\"text-generation\",\n",
    "    max_new_tokens=50,\n",
    "    do_sample=False,\n",
    "    huggingfacehub_api_token=hf_api\n",
    ")\n",
    "\n",
    "llm.invoke(\"is victoria a lovely name ?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Langchain PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"is victoria a lovely name ?\"\n",
    "\n",
    "template = \"\"\"\n",
    "Question: {question}\n",
    "Answer: answer in french\n",
    "\"\"\"\n",
    "prompt = PromptTemplate(template=template, input_variables=question)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': 'is victoria a lovely name ?',\n",
       " 'text': 'Réponse: Oui, Victoria est un très beau prénom! Il est très populaire et a été porté par plusieurs femmes célèbres, notamment la reine Victoria du Royaume-Uni. Il a également un certain char'}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_chain = LLMChain(llm=llm, prompt=prompt)\n",
    "llm_chain.invoke(question)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HF Pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to facebook/bart-large-mnli and revision c626438 (https://huggingface.co/facebook/bart-large-mnli).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "/Users/marinneyret/Desktop/LLMapp/.venv/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'ZeroShotClassificationPipeline' object has no attribute 'dtype'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[31], line 6\u001b[0m\n\u001b[1;32m      1\u001b[0m classifier_pipe \u001b[38;5;241m=\u001b[39m pipeline(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mzero-shot-classification\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      2\u001b[0m classifier_pipe(\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis is a course about the Transformers library\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      4\u001b[0m     candidate_labels\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtech\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbusiness\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m      5\u001b[0m )\n\u001b[0;32m----> 6\u001b[0m \u001b[43mclassifier_pipe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdtype\u001b[49m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'ZeroShotClassificationPipeline' object has no attribute 'dtype'"
     ]
    }
   ],
   "source": [
    "classifier_pipe = pipeline(\"zero-shot-classification\")\n",
    "classifier_pipe(\n",
    "    \"This is a course about the Transformers library\",\n",
    "    candidate_labels=[\"tech\", \"business\"],\n",
    ")\n",
    "classifier_pipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'translation_text': 'Ich bin Englisch'}]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translator_pipe = pipeline(\"translation_en_to_fr\", model=\"google-t5/t5-small\")\n",
    "translator_pipe(\"I am english\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HF AUTO LIB\n",
    "- pipeline() fonction is a combinaison of AutoTokenizer and AutoModel that works under the hood."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at nlptown/bert-base-multilingual-uncased-sentiment were not used when initializing TFBertForSequenceClassification: ['dropout_37']\n",
      "- This IS expected if you are initializing TFBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFBertForSequenceClassification were initialized from the model checkpoint at nlptown/bert-base-multilingual-uncased-sentiment.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForSequenceClassification for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, TFAutoModelForSequenceClassification\n",
    "\n",
    "model_name = \"nlptown/bert-base-multilingual-uncased-sentiment\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = TFAutoModelForSequenceClassification.from_pretrained(model_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 5), dtype=float32, numpy=\n",
       "array([[0.00731268, 0.00752774, 0.06288627, 0.24432226, 0.67795104],\n",
       "       [0.5660091 , 0.24136913, 0.10153452, 0.03901039, 0.05207687]],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_prompt = tokenizer([\"je suis une enfant qui adore Paris\", \"et moi je deteste Paris\"],\n",
    "                    padding=True,\n",
    "                    truncation=True,\n",
    "                    max_length=512,\n",
    "                    return_tensors=\"tf\")\n",
    "\n",
    "tf_outputs = model(tokenized_prompt)\n",
    "\n",
    "tf_predictions = tf.nn.softmax(tf_outputs.logits, axis=-1)\n",
    "tf_predictions"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
